{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "    # Use of this source code is governed by an MIT-style\n",
    "    # license that can be found in the LICENSE file or at\n",
    "    # https://opensource.org/licenses/MIT.\n",
    "\n",
    "    # Author(s): Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://opensource.org/licenses/MIT\" target=\"_parent\"><img src=\"https://img.shields.io/github/license/probml/pyprobml\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/book1/figures/chapter10_logistic_regression_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.1:<a name='10.1'></a> <a name='iris-logreg-2d'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  (a) Visualization of a 2d plane in a 3d space with surface normal $\\mathbf  w $ going through point $\\mathbf  x _0=(x_0,y_0,z_0)$. See text for details. (b) Visualization of optimal linear decision boundary induced by logistic regression on a 2-class, 2-feature version of the iris dataset.  \n",
    "Figure(s) generated by [iris_logreg.py](https://github.com/probml/pyprobml/blob/master/scripts/iris_logreg.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_and_run(\"/pyprobml/scripts/iris_logreg.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.2:<a name='10.2'></a> <a name='sigmoidPlot2d'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Plots of $\\sigma (w_1 x_1 + w_2 x_2)$. Here $\\mathbf  w = (w_1,w_2)$ defines the normal to the decision boundary. Points to the right of this have $\\sigma (\\mathbf  w ^\\top \\mathbf  x )>0.5$, and points to the left have $\\sigma (\\mathbf  w ^\\top \\mathbf  x ) < 0.5$. Adapted from Figure 39.3 of <a href='#MacKay03'>[Mac03]</a> .  \n",
    "Figure(s) generated by [sigmoid_2d_plot.py](https://github.com/probml/pyprobml/blob/master/scripts/sigmoid_2d_plot.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_and_run(\"/pyprobml/scripts/sigmoid_2d_plot.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.3:<a name='10.3'></a> <a name='kernelTrickQuadratic'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of how we can transform a quadratic decision boundary into a linear one by transforming the features from $\\mathbf  x =(x_1,x_2)$ to $\\boldsymbol  \\phi  (\\mathbf  x )=(x_1^2,x_2^2)$. Used with kind permission of Jean-Philippe Vert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_image(\"/pyprobml/book1/figures/images/Figure_10.3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.4:<a name='10.4'></a> <a name='logregPoly'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Polynomial feature expansion applied to a two-class, two-dimensional logistic regression problem. (a) Degree $K=1$. (a) Degree $K=2$. (a) Degree $K=4$. (d) Train and test error vs degree.  \n",
    "Figure(s) generated by [logreg_poly_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/logreg_poly_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_and_run(\"/pyprobml/scripts/logreg_poly_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.5:<a name='10.5'></a> <a name='irisLossSurface'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  NLL loss surface for binary logistic regression applied to Iris dataset with 1 feature and 1 bias term. The goal is to minimize the function.  \n",
    "Figure(s) generated by [iris_logreg_loss_surface.py](https://github.com/probml/pyprobml/blob/master/scripts/iris_logreg_loss_surface.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_and_run(\"/pyprobml/scripts/iris_logreg_loss_surface.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.6:<a name='10.6'></a> <a name='logregPolyRidge'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Weight decay with variance $C$ applied to two-class, two-dimensional logistic regression problem with a degree 4 polynomial. (a) $C=1$. (a) $C=316$. (a) $C=100,000$. (d) Train and test error vs $C$.  \n",
    "Figure(s) generated by [logreg_poly_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/logreg_poly_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_and_run(\"/pyprobml/scripts/logreg_poly_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.7:<a name='10.7'></a> <a name='logregMultinom3class'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Example of 3-class logistic regression with 2d inputs. (a) Original features. (b) Quadratic features.  \n",
    "Figure(s) generated by [logreg_multiclass_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/logreg_multiclass_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_and_run(\"/pyprobml/scripts/logreg_multiclass_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.8:<a name='10.8'></a> <a name='labelTree'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  A simple example of a label hierarchy. Nodes within the same ellipse have a mutual exclusion relationship between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_image(\"/pyprobml/book1/figures/images/Figure_10.8.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.9:<a name='10.9'></a> <a name='hierSoftmax'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  A flat and hierarchical softmax model $p(w|C)$, where $C$ are the input features (context) and $w$ is the output label (word). Adapted from   https://www.quora.com/What-is-hierarchical-softmax . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_image(\"/pyprobml/book1/figures/images/Figure_10.9_A.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_image(\"/pyprobml/book1/figures/images/Figure_10.9_B.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.10:<a name='10.10'></a> <a name='logregRobust'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  (a) Logistic regression on some data with outliers (denoted by x). Training points have been (vertically) jittered to avoid overlapping too much. Vertical line is the decision boundary, and its posterior credible interval. (b) Same as (a) but using robust model, with a mixture likelihood. Adapted from Figure 4.13 of <a href='#Martin2018'>[Mar18]</a> .  \n",
    "Figure(s) generated by [logreg_iris_bayes_robust_1d_pymc3.py](https://github.com/probml/pyprobml/blob/master/scripts/logreg_iris_bayes_robust_1d_pymc3.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_and_run(\"/pyprobml/scripts/logreg_iris_bayes_robust_1d_pymc3.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.11:<a name='10.11'></a> <a name='bitemperedLoss'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  (a) Illustration of logistic and tempered logistic loss with $t_1=0.8$. (b) Illustration of sigmoid and tempered sigmoid transfer function with $t_2=2.0$. From   https://ai.googleblog.com/2019/08/bi-tempered-logistic-loss-for-training.html . Used with kind permission of Ehsan Amid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_image(\"/pyprobml/book1/figures/images/Figure_10.11_A.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_image(\"/pyprobml/book1/figures/images/Figure_10.11_B.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.12:<a name='10.12'></a> <a name='bitempered'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Illustration of standard and bi-tempered logistic regression on data with label noise. From   https://ai.googleblog.com/2019/08/bi-tempered-logistic-loss-for-training.html . Used with kind permission of Ehsan Amid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_image(\"/pyprobml/book1/figures/images/Figure_10.12.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.13:<a name='10.13'></a> <a name='logregLaplaceGirolamiPost'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  (a) Illustration of the data. (b) Log-likelihood for a logistic regression model. The line is drawn from the origin in the direction of the MLE (which is at infinity). The numbers correspond to 4 points in parameter space, corresponding to the lines in (a). (c) Unnormalized log posterior (assuming vague spherical prior). (d) Laplace approximation to posterior. Adapted from a figure by Mark Girolami.  \n",
    "Figure(s) generated by [logregLaplaceGirolamiDemo.py](https://github.com/probml/pyprobml/blob/master/scripts/logregLaplaceGirolamiDemo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_and_run(\"/pyprobml/scripts/logregLaplaceGirolamiDemo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.14:<a name='10.14'></a> <a name='logregLaplaceDemoPred'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Posterior predictive distribution for a logistic regression model in 2d. Top left: contours of $p(y=1|\\mathbf  x , \\mathbf  w  _ map )$. Top right: samples from the posterior predictive distribution. Bottom left: Averaging over these samples. Bottom right: moderated output (probit approximation). Adapted from a figure by Mark Girolami.  \n",
    "Figure(s) generated by [logregLaplaceGirolamiDemo.py](https://github.com/probml/pyprobml/blob/master/scripts/logregLaplaceGirolamiDemo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_and_run(\"/pyprobml/scripts/logregLaplaceGirolamiDemo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.15:<a name='10.15'></a> <a name='ridgeLassoOLS'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  (a) Data for logistic regression question. (b) Plot of $ w _k$ vs amount of correlation $c_k$ for three different estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #@title Setup { display-mode: \"form\" }\n",
    "%%time\n",
    "# If you run this for the first time it would take ~25/30 seconds\n",
    "!git clone https://github.com/probml/pyprobml /pyprobml &> /dev/null && git clone https://github.com/probml/colab_powertoys.git &> /dev/null \n",
    "!pip3 install nbimporter -qqq \n",
    "%cd -q /content/colab_powertoys \n",
    "from colab_powertoys.probml_toys import probml_toys as pmlt\n",
    "%cd -q /pyprobml/scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_image(\"/pyprobml/book1/figures/images/Figure_10.15_A.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmlt.show_image(\"/pyprobml/book1/figures/images/Figure_10.15_B.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    " <a name='MacKay03'>[Mac03]</a> D. MacKay \"Information Theory, Inference, and Learning Algorithms\". (2003). \n",
    "\n",
    "<a name='Martin2018'>[Mar18]</a> O. Martin \"Bayesian analysis with Python\". (2018). \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
